# -*- coding: utf-8 -*-
"""lemidahishuvit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zons8HHzCTh7inQGEODeNuXT3hETbYiH
"""

!pip install catboost
import statistics
# # Install a pip package in the current Jupyter kernel
# import sys
# !{sys.executable} -m pip install numpy
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE, SVMSMOTE
from sklearn import svm
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge, LinearRegression
from sklearn.metrics import f1_score, accuracy_score, classification_report, roc_auc_score, roc_curve, confusion_matrix
from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, train_test_split, cross_val_score, \
    ShuffleSplit
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from tqdm import tqdm
import xgboost as xgb
from matplotlib import pyplot
from catboost import CatBoostClassifier
import lightgbm as lgb
import warnings
warnings.filterwarnings("ignore")
warnings.filterwarnings("ignore", message="the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'")

def fill_missing_values(df_to_fill):
    df_to_fill.dropna(how='all')
    df_to_fill = df_to_fill.apply(lambda x: x.fillna(x.mean()),axis=0)
    print(df_to_fill)
    return df_to_fill

def pre_processing(df_in):
    preproc_df=df_in
    print("Start Pre-processing")
    to_drop = ['Patient ID',
               'Patient age quantile',
               'Patient addmited to regular ward (1=yes, 0=no)',
               'Patient addmited to semi-intensive unit (1=yes, 0=no)',
               'Patient addmited to intensive care unit (1=yes, 0=no)',
               'Urine - Esterase',
               'Urine - pH',
               'Urine - Hemoglobin',
               'Urine - Bile pigments',
               'Urine - Ketone Bodies',
               'Urine - Nitrite',
               'Urine - Density',
               'Urine - Urobilinogen',
               'Urine - Protein',
               'Urine - Sugar',
               'Urine - Leukocytes',
               'Urine - Crystals',
               'Urine - Red blood cells',
               'Urine - Hyaline cylinders',
               'Urine - Granular cylinders',
               'Urine - Yeasts',
               'Urine - Color',
               'Influenza A',
               'Influenza B',
               'Parainfluenza 1',
               'Parainfluenza 3',
               'Parainfluenza 2',
               'Parainfluenza 4',
               'Respiratory Syncytial Virus',
               'CoronavirusNL63',
               'Coronavirus HKU1',
               'Coronavirus229E',
               'CoronavirusOC43',
               'Inf A H1N1 2009',
               'Influenza B, rapid test',
               'Influenza A, rapid test',
               'Strepto A',
               'Metapneumovirus',
               'Adenovirus',
               'Chlamydophila pneumoniae',
               'Bordetella pertussis',
               'Rhinovirus/Enterovirus']

    preproc_df.drop(to_drop, inplace=True, axis=1)

    pct_null = preproc_df.isnull().sum() / len(preproc_df)
    missing_features = pct_null[pct_null > 0.95].index
    preproc_df.drop(missing_features, axis=1, inplace=True)

    # imputer = IterativeImputer(BayesianRidge())
    # impute_data = pd.DataFrame(imputer.fit_transform(df))

    preproc_df["SARS-Cov-2 exam result"].replace(to_replace=['positive', 'negative'], value=[1, 0], inplace=True)
    columns = preproc_df.columns.to_list()
    columns.remove('SARS-Cov-2 exam result')
    preproc_df = preproc_df.dropna(subset=columns, axis=0, how='all')
    #     df = df.apply(lambda x: x.fillna(x.mean()),axis=0)
    print("Done pre-processing")
    return preproc_df

def evaluation_models(data_in):
    print("Start evaluate models using nested cross-validation")
    data = data_in
    y = data["SARS-Cov-2 exam result"].to_numpy()
    X = data.drop("SARS-Cov-2 exam result", axis=1).to_numpy()

    models_to_run = [RandomForestClassifier(), xgb.XGBClassifier(use_label_encoder=False, verbosity=0),
                     LogisticRegression(), svm.SVC(), MLPClassifier()]
    models_param_grid = [
        {  # 1st param grid, corresponding to RandomForestClassifier
            'max_depth': [2, 4, 8, 16, 32, 64],
            'n_estimators': [10, 20, 30, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100],
        },
        {  # 2nd param grid, corresponding to XGBClassifier
            'learning_rate': [0.1, 0.05, 0.01],
            'max_depth': [2, 4, 8, 16, 32, 64],
            'n_estimators': [10, 20, 30, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100],
        },
        {  # 3rd param grid, corresponding to Linear Regression

        },
        {  # 4 param grid, corresponding to SVC
            'kernel': ['rbf', 'linear'],
            'C': [0.001, 0.01, 0.05, 0.1, 0.5, 1, 10],
            'gamma': [0.001, 0.01, 0.05, 0.1, 0.5, 1, 10]

        },
        {  # 5 param grid, corresponding to MLP
            'hidden_layer_sizes': [(64, 32, 16), (32, 32, 16), (64, 32, 32), (64, 64, 32, 32)],
            'alpha': [0.01, 0.05, 0.005, 0.001, 0.0001],
            'learning_rate': ['adaptive'],
        }
    ]
    best_models = []
    model_accuracies = []
    for i, model in enumerate(models_to_run):
        print("Evaluate ", model)
        # configure the cross-validation procedure
        cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)
        # enumerate splits
        outer_results = list()
        accuracies = list()
        for train_ix, test_ix in cv_outer.split(X):
            # split data
            X_train, X_test = X[train_ix], X[test_ix]
            y_train, y_test = y[train_ix], y[test_ix]
            imp = IterativeImputer()
            X_train = imp.fit_transform(X_train)
            X_test = imp.transform(X_test)
            # using SMOTE
            smote = SVMSMOTE(random_state=101, k_neighbors=5)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            if (i != 2):
                # configure the cross-validation procedure
                cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)
                # define search
                search = GridSearchCV(models_to_run[i], models_param_grid[i], scoring='f1', cv=cv_inner, refit=True)
                # execute search
                result = search.fit(X_train, y_train)
                # get the best performing model fit on the whole training set
                best_model = result.best_estimator_
            # in case of LinearRegression
            elif (i == 2):
                best_model = models_to_run[i].fit(X_train, y_train)
            # evaluate model on the hold out dataset
            yhat = best_model.predict(X_test)
            # evaluate the model
            accuracy = accuracy_score(y_test, yhat)
            accuracies.append(accuracy)
            # save a tuple for each running - the model and it's accuracy
            tuple_to_add = (accuracy, best_model)
            # store the result
            outer_results.append(tuple_to_add)
        model_accuracies.append(np.mean(accuracies))
        # check what model brings the best accuracy
        max_accuracy = 0
        for tup in outer_results:
            if tup[0] > max_accuracy:
                best_model = tup[1]
                max_accuracy = tup[0]
        # save the best model
        # the i place in the array represent the i model as declared before
        best_models.append(best_model)
        print("Chosen best model:", best_model)

    return best_models, model_accuracies

def runWithBestParams(data_in, best_Models, model_accuracies, models_scores):
    data = data_in
    y = data["SARS-Cov-2 exam result"]
    X = data.drop("SARS-Cov-2 exam result", axis=1).to_numpy()
    print("Run 10 times each model with best params")
    # run for each algorithm 10 times, each time with different 80% of training set
    for index, model in enumerate(best_Models):
        print("Run model ", model)
        accuracies = []
        f1Scores = []
        sensitivities = []
        specificities = []
        roc_auc = []
        for k in range(10):
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
            imp = IterativeImputer()
            X_train = imp.fit_transform(X_train)
            X_test = imp.transform(X_test)
            smote = SVMSMOTE(random_state=101, k_neighbors=5)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            model.fit(X_train, y_train)
            prediction = model.predict(X_test)
            accuracies.append(accuracy_score(y_test, prediction))
            f1Scores.append(f1_score(y_test, prediction))
            tn, fp, fn, tp = confusion_matrix(y_test, prediction).ravel()
            sensitivities.append(tp / (tp + fn))
            specificities.append(tn / (tn + fp))
            roc_auc.append(roc_auc_score(y_test.array, prediction))

        models_scores[index]['accuracy'] = (statistics.mean(accuracies), statistics.stdev(accuracies))
        models_scores[index]['f1Score'] = (statistics.mean(f1Scores), statistics.stdev(f1Scores))
        models_scores[index]['sensitivity'] = (statistics.mean(sensitivities), statistics.stdev(sensitivities))
        models_scores[index]['specificity'] = (statistics.mean(specificities), statistics.stdev(specificities))
        models_scores[index]['roc_auc'] = (statistics.mean(roc_auc), statistics.stdev(roc_auc))
        print("Results for model ", model)
        print(models_scores[index])

def runEnsemble(best_models, model_accuracies, data_in, models_scores):
    # preparing data
    data = data_in
    y = data["SARS-Cov-2 exam result"].to_numpy()
    X = data.drop("SARS-Cov-2 exam result", axis=1).to_numpy()
    print("Run ensemble algorithm")
    # prepare weights
    weights = np.empty(5)
    weights.fill(1)
    # increase weight of most accurate model
    max_value = max(model_accuracies)
    max_index = model_accuracies.index(max_value)
    weights[max_index] = 2
    # decrease weight of least accurate model
    min_value = min(model_accuracies)
    min_index = model_accuracies.index(min_value)
    weights[min_index] = 0

    # split data to train and test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    imp = IterativeImputer()
    X_train = imp.fit_transform(X_train)
    X_test = imp.transform(X_test)
    smote = SVMSMOTE(random_state=101, k_neighbors=5)
    X_train, y_train = smote.fit_resample(X_train, y_train)
    accuracies = []
    f1Scores = []
    sensitivities = []
    specificities = []
    roc_auc = []
    for o in range(10):
        predictions = []
        for model in best_models:
            model.fit(X_train, y_train)
            prediction = model.predict(X_test)
            predictions.append(prediction)
        final_prediction = []
        for j in range(len(y_test)):  # run on all rows
            one_counter = 0
            zero_counter = 0
            for t in range(5):  # run on each prediction of each algorithm in specific row
                if predictions[t][j] == 1:
                    one_counter = one_counter + weights[t] * 1
                else:
                    zero_counter = zero_counter + weights[t] * 1
            if one_counter > zero_counter:
                final_prediction.append(1)
            else:
                final_prediction.append(0)

        accuracies.append(accuracy_score(y_test, final_prediction))
        f1Scores.append(f1_score(y_test, final_prediction))
        tn, fp, fn, tp = confusion_matrix(y_test, final_prediction).ravel()
        sensitivities.append(tp / (tp + fn))
        specificities.append(tn / (tn + fp))
        roc_auc.append(roc_auc_score(y_test, prediction))
    models_scores[5]['accuracy'] = (statistics.mean(accuracies), statistics.stdev(accuracies))
    models_scores[5]['f1Score'] = (statistics.mean(f1Scores), statistics.stdev(f1Scores))
    models_scores[5]['sensitivity'] = (statistics.mean(sensitivities), statistics.stdev(sensitivities))
    models_scores[5]['specificity'] = (statistics.mean(specificities), statistics.stdev(specificities))
    models_scores[5]['roc_auc'] = (statistics.mean(roc_auc), statistics.stdev(roc_auc))
    print("Ensemble results: ", models_scores[5])

# pre-process data
df = pd.read_csv('../input/dataset/dataset.csv')
data_in = pre_processing(df)

# evaluation using nested cross validation for 5 models
print("start evaluation of models using nested cross validatin")
best_models, model_accuracies = evaluation_models(data_in)

# running 10 times with best hyperparameters
models_scores = {}
for i in range(6):
    models_scores[i] = {}
print("run each chosen model 10 times")
runWithBestParams(data_in, best_models, model_accuracies, models_scores)

print("run ensemble 10 times")
runEnsemble(best_models, model_accuracies, data_in, models_scores)

# feature engineering
print("running new 5 features")
df_copy = data_in.copy()
df_copy = df_copy.reset_index()
del df_copy['index']
df_copy['hematocrit_redBloodCells_ratio'] = df_copy['Hematocrit']/df_copy['Red blood Cells']
# df_copy['Platelets_MPV_difference'] = df_copy['Platelets']- df_copy['Mean platelet volume ']
df_copy['Platelets_MPV_difference'] = df_copy['Platelets']/df_copy['Mean platelet volume ']
df_copy['Leukocytes_square'] = df_copy[['Leukocytes']].apply(lambda x: x * x)
df_copy['MCHC_hemoglobin_ratio'] = df_copy['Mean corpuscular hemoglobin concentration (MCHC)'] / df_copy['Hemoglobin']
df_copy['Monocytes_polinom'] = df_copy[['Monocytes']].apply(lambda x: 8 * x * x + 5 * x + 1)
# evaluation using nested cross validation for 5 models
best_models_new_features, model_accuracies_new_features = evaluation_models(df_copy)
# running 10 times with best hyperparameters
models_scores_new_features = {}
for i in range(6):
    models_scores_new_features[i] = {}
runWithBestParams(df_copy, best_models_new_features, model_accuracies_new_features, models_scores_new_features)
# run ensemble
runEnsemble(best_models_new_features, model_accuracies_new_features, data_in, models_scores_new_features)

def evaluation_CatBoost_and_LightGBM(data_in):
    print("Evaluate CatBoost and LightGBM")
    data = data_in
    y = data["SARS-Cov-2 exam result"].to_numpy()
    X = data.drop("SARS-Cov-2 exam result", axis=1).to_numpy()

    models_to_run = [CatBoostClassifier(silent=True), lgb.LGBMClassifier()]
    models_param_grid = [
        {

        },
        {

        }
    ]
    best_models = []
    model_accuracies = []
    for i, model in enumerate(models_to_run):
        print("Evaluate model ", model)
        # configure the cross-validation procedure
        cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)
        # enumerate splits
        outer_results = list()
        accuracies = list()
        for train_ix, test_ix in cv_outer.split(X):
            # split data
            X_train, X_test = X[train_ix], X[test_ix]
            y_train, y_test = y[train_ix], y[test_ix]
            imp = IterativeImputer()
            X_train = imp.fit_transform(X_train)
            X_test = imp.transform(X_test)
            # using SMOTE
            smote = SVMSMOTE(random_state=101, k_neighbors=5)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            # configure the cross-validation procedure
            cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)
            # define search
            search = GridSearchCV(models_to_run[i], models_param_grid[i], scoring='f1', cv=cv_inner, refit=True)
            # execute search
            result = search.fit(X_train, y_train)
            # get the best performing model fit on the whole training set
            best_model = result.best_estimator_
            # evaluate model on the hold out dataset
            yhat = best_model.predict(X_test)
            # evaluate the model
            accuracy = accuracy_score(y_test, yhat)
            accuracies.append(accuracy)
            # save a tuple for each running - the model and it's accuracy
            tuple_to_add = (accuracy, best_model)
            # store the result
            outer_results.append(tuple_to_add)
        model_accuracies.append(np.mean(accuracies))
        # check what model brings the best accuracy
        max_accuracy = 0
        for tup in outer_results:
            if tup[0] > max_accuracy:
                best_model_CatBoost_and_LightGBM = tup[1]
                max_accuracy = tup[0]
        # save the best model
        # the i place in the array represent the i model as declared before
        best_models.append(best_model_CatBoost_and_LightGBM)
        print("Chosen model: ", best_model_CatBoost_and_LightGBM)

    return best_models

def runWithBestParams_CatBoost_and_LightGBM(data_in, best_Models, models_scores):
    data = data_in
    y = data["SARS-Cov-2 exam result"]
    X = data.drop("SARS-Cov-2 exam result", axis=1).to_numpy()
    print("Run 10 times each model with best params")
    # run for each algorithm 10 times, each time with different 80% of training set
    for index, model in enumerate(best_Models):
        accuracies = []
        f1Scores = []
        sensitivities = []
        specificities = []
        roc_auc = []
        for k in range(10):
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
            imp = IterativeImputer()
            X_train = imp.fit_transform(X_train)
            X_test = imp.transform(X_test)
            smote = SVMSMOTE(random_state=101, k_neighbors=5)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            model.fit(X_train, y_train)
            prediction = model.predict(X_test)
            accuracies.append(accuracy_score(y_test, prediction))
            f1Scores.append(f1_score(y_test, prediction))
            tn, fp, fn, tp = confusion_matrix(y_test, prediction).ravel()
            sensitivities.append(tp / (tp + fn))
            specificities.append(tn / (tn + fp))
            roc_auc.append(roc_auc_score(y_test, prediction))

        models_scores[index]['accuracy'] = (statistics.mean(accuracies), statistics.stdev(accuracies))
        models_scores[index]['f1Score'] = (statistics.mean(f1Scores), statistics.stdev(f1Scores))
        models_scores[index]['sensitivity'] = (statistics.mean(sensitivities), statistics.stdev(sensitivities))
        models_scores[index]['specificity'] = (statistics.mean(specificities), statistics.stdev(specificities))
        models_scores[index]['roc_auc'] = (statistics.mean(roc_auc), statistics.stdev(roc_auc))
        print("Results For model ", model)
        print(models_scores[index])

# run CatBoost and LightGBM
# evaluation using nested cross validation for 2 models
print("running CatBoost and LightGBM using nested cross-validation")
best_models_CatBoost_and_LightGBM = evaluation_CatBoost_and_LightGBM(data_in)

# running 10 times with best hyperparameters
print("running 10 times CatBoost and LightGBM")
models_scores_CatBoost_and_LightGBM = {}
for i in range(2):
    models_scores_CatBoost_and_LightGBM[i] = {}
runWithBestParams_CatBoost_and_LightGBM(data_in, best_models_CatBoost_and_LightGBM, models_scores_CatBoost_and_LightGBM)

"""**Question 6**"""

data_witout_null = data_in
data_witout_null = fill_missing_values(data_witout_null)
data_witout_null

import pandas as pd
import numpy as np
np.random.seed(0)
import matplotlib.pyplot as plt
df = data_witout_null
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
# The target variable is '"SARS-Cov-2 exam result"'.

Y = df["SARS-Cov-2 exam result"]
X =  df[['Hematocrit', 'Hemoglobin', 'Platelets', 'Mean platelet volume ', 'Red blood Cells',
         'Lymphocytes', 'Mean corpuscular hemoglobin concentration (MCHC)', 'Leukocytes',  
         'Basophils', 'Mean corpuscular hemoglobin (MCH)', 'Eosinophils',
       'Mean corpuscular volume (MCV)', 'Monocytes',       'Red blood cell distribution width (RDW)',
        'Neutrophils', 'Urea',       'Proteina C reativa mg/dL', 'Creatinine', 'Potassium', 'Sodium']]
# Split the data into train and test data:
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)

!pip install shap==0.36.0

import shap

"""**RandomForestRegressor**"""

rf = RandomForestRegressor(max_depth=32,n_estimators=20)
rf.fit(X_train, Y_train)  
print(rf.feature_importances_)
f = plt.figure()
importances = rf.feature_importances_
indices = np.argsort(importances)
features = X_train.columns
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
f.savefig("/tmp/yuval/Feature Importances/RF.png", bbox_inches='tight', dpi=600)

import shap
rf_shap_values = shap.KernelExplainer(rf.predict,X_test)

rf_shap_values = rf_shap_values.shap_values( X_test)

import matplotlib.pyplot as plt
f = plt.figure()
shap.summary_plot(rf_shap_values, X_test)
f.savefig("/tmp/yuval/RF.png", bbox_inches='tight', dpi=600)

"""**XGBoostClassifier**"""

# xgboost
import xgboost

XGBC = xgboost.XGBRegressor(learning_rate=0.1, max_depth =32, n_estimators = 70)
XGBC.fit(X_train.values, Y_train.values)  
print(XGBC.feature_importances_)
f = plt.figure()
importances = XGBC.feature_importances_
indices = np.argsort(importances)
features = X_train.columns
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

f.savefig("/tmp/yuval/Feature Importances/xgboost.png", bbox_inches='tight', dpi=600)

XGBC =shap.KernelExplainer(XGBC.predict,X_test)

XGBC_shap_values = XGBC.shap_values( X_test)

import matplotlib.pyplot as plt
f = plt.figure()
shap.summary_plot(XGBC_shap_values, X_test)
f.savefig("/tmp/yuval/xgboost.png", bbox_inches='tight', dpi=600)

"""**CatBoost**"""

# Catboost 
CatBoost = CatBoostClassifier(silent=True)
CatBoost = CatBoost.fit(X_train, Y_train)  
print(CatBoost.feature_importances_)

f = plt.figure()
importances = CatBoost.feature_importances_
indices = np.argsort(importances)
features = X_train.columns
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
f.savefig("/tmp/yuval/Feature Importances/CatBoost.png", bbox_inches='tight', dpi=600)

CatBoost_shap_values = shap.KernelExplainer(CatBoost.predict,X_test)

CatBoost_shap_values = CatBoost_shap_values.shap_values( X_test)

import matplotlib.pyplot as plt
f = plt.figure()
shap.summary_plot(CatBoost_shap_values, X_test)
f.savefig("/tmp/yuval/CatBoost.png", bbox_inches='tight', dpi=600)

"""**LightGBM**"""

# LightGBM 
import lightgbm as lgb

lgbm = lgb.LGBMClassifier()
lgbm =  CatBoost.fit(X_train, Y_train)    
print(lgbm.feature_importances_)
f = plt.figure()
importances = lgbm.feature_importances_
indices = np.argsort(importances)
features = X_train.columns
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
f.savefig("/tmp/yuval/Feature Importances/LightGBM.png", bbox_inches='tight', dpi=600)

lgbm_shap_values = shap.KernelExplainer(lgbm.predict,X_test)

lgbm_shap_values = lgbm_shap_values.shap_values( X_test)

import matplotlib.pyplot as plt
f = plt.figure()
shap.summary_plot(lgbm_shap_values, X_test)
f.savefig("/tmp/yuval/LightGBM.png", bbox_inches='tight', dpi=600)